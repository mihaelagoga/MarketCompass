{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9134d-7c62-4dc6-bdb7-265c914b0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "import copy\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TARGET_COL = 'target'\n",
    "COLS_TO_DROP = ['date']\n",
    "LOOKBACK_DAYS = 5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loads the pre-split data from CSV files.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        train_df = pd.read_csv('train_data.csv')\n",
    "        test_df = pd.read_csv('test_data.csv')\n",
    "        val_df = pd.read_csv('validation_data.csv')\n",
    "        \n",
    "        X_train = train_df.drop(columns=[TARGET_COL])\n",
    "        y_train = train_df[TARGET_COL]\n",
    "        \n",
    "        X_test = test_df.drop(columns=[TARGET_COL])\n",
    "        y_test = test_df[TARGET_COL]\n",
    "        \n",
    "        X_val = val_df.drop(columns=[TARGET_COL])\n",
    "        y_val = val_df[TARGET_COL]\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Did you run 'process_market_data.py' first?\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def define_feature_cols(df: pd.DataFrame) -> (list, list):\n",
    "    \"\"\"\n",
    "    Identifies numerical and categorical features to be processed.\n",
    "    \"\"\"\n",
    "    features = [col for col in df.columns if col not in COLS_TO_DROP]\n",
    "    \n",
    "    categorical_cols = ['day_of_week', 'fiscal_week']\n",
    "    \n",
    "    numerical_cols = [col for col in features if col not in categorical_cols]\n",
    "    \n",
    "    print(f\"Identified {len(numerical_cols)} numerical features.\")\n",
    "    print(f\"Identified {len(categorical_cols)} categorical features.\")\n",
    "    \n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "def build_preprocessing_pipeline(numerical_cols: list, categorical_cols: list) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Creates a scikit-learn pipeline to process features:\n",
    "    - Numerical: StandardScaler\n",
    "    - Categorical: OneHotEncoder\n",
    "    \"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def evaluate_model(name: str, y_true: pd.Series, y_pred: np.ndarray, y_prob: np.ndarray = None):\n",
    "    \"\"\"Prints a standard set of classification metrics.\"\"\"\n",
    "    print(f\"\\n--- Evaluation Report: {name} ---\")\n",
    "    \n",
    "    if len(y_true) == 0 or len(y_pred) == 0:\n",
    "        print(\"Not enough data to evaluate (empty y_true or y_pred).\")\n",
    "        return\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_prob)\n",
    "            print(f\"ROC-AUC:  {auc:.4f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not calculate ROC-AUC: {e}\")\n",
    "            \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(\"-\" * (30 + len(name)))\n",
    "\n",
    "def create_sequences(X: np.ndarray, y: np.ndarray, lookback: int) -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Converts 2D feature/target data into 3D sequences for RNNs.\n",
    "    Returns PyTorch Tensors.\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    if len(X) <= lookback:\n",
    "        return torch.empty(0, lookback, X.shape[1]), torch.empty(0)\n",
    "        \n",
    "    for i in range(lookback, len(X)):\n",
    "        X_seq.append(X[i-lookback:i, :])\n",
    "        y_seq.append(y[i])\n",
    "        \n",
    "    X_seq_np = np.array(X_seq, dtype=np.float32)\n",
    "    y_seq_np = np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "    return torch.from_numpy(X_seq_np), torch.from_numpy(y_seq_np)\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_features, gru_units=32, dropout_rate=0.3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_features,\n",
    "            hidden_size=gru_units,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(gru_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, hidden = self.gru(x)\n",
    "        \n",
    "        last_hidden_state = hidden.squeeze(0)\n",
    "        \n",
    "        x = self.relu(last_hidden_state)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def train_pytorch_model(model, train_loader, val_loader, loss_fn, optimizer, epochs, patience, device):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred_logits = model(X_batch).squeeze()\n",
    "            \n",
    "            loss = loss_fn(y_pred_logits, y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch_val, y_batch_val in val_loader:\n",
    "                X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
    "                \n",
    "                y_val_logits = model(X_batch_val).squeeze()\n",
    "                loss = loss_fn(y_val_logits, y_batch_val)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "             print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                   f\"Train Loss: {avg_train_loss:.4f}.. \"\n",
    "                   f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} with best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "            \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    print(\"GRU training complete.\")\n",
    "\n",
    "def predict_pytorch_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in loader:\n",
    "            X_batch = X_batch[0].to(device)\n",
    "            \n",
    "            y_logits = model(X_batch)\n",
    "            y_probs = torch.sigmoid(y_logits).squeeze()\n",
    "            \n",
    "            all_probs.append(y_probs.cpu().numpy())\n",
    "            \n",
    "    if not all_probs:\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_preds = (all_probs > 0.5).astype(int)\n",
    "    \n",
    "    return all_preds, all_probs\n",
    "\n",
    "def main():\n",
    "    data = load_data()\n",
    "    if data[0] is None:\n",
    "        return\n",
    "        \n",
    "    X_train, y_train, X_test, y_test, X_val, y_val = data\n",
    "    \n",
    "    numerical_cols, categorical_cols = define_feature_cols(X_train)\n",
    "    preprocessor = build_preprocessing_pipeline(numerical_cols, categorical_cols)\n",
    "\n",
    "    print(\"\\nTraining Model 1: Logistic Regression...\")\n",
    "    logit_pipeline = Pipeline(steps=[\n",
    "        ('preprocess', preprocessor),\n",
    "        ('model', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    logit_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_logit = logit_pipeline.predict(X_test)\n",
    "    y_prob_logit = logit_pipeline.predict_proba(X_test)[:, 1]\n",
    "    evaluate_model(\"Logistic Regression (Test Set)\", y_test, y_pred_logit, y_prob_logit)\n",
    "\n",
    "    if (y_train == 1).sum() > 0:\n",
    "        scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    else:\n",
    "        scale_pos_weight = 1\n",
    "\n",
    "    xgb_pipeline = Pipeline(steps=[\n",
    "        ('preprocess', preprocessor),\n",
    "        ('model', XGBClassifier(use_label_encoder=False, \n",
    "                                eval_metric='logloss', \n",
    "                                random_state=42,\n",
    "                                scale_pos_weight=scale_pos_weight\n",
    "                               ))\n",
    "    ])\n",
    "    \n",
    "    xgb_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "    y_prob_xgb = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "    evaluate_model(\"XGBoost (Test Set)\", y_test, y_pred_xgb, y_prob_xgb)\n",
    "\n",
    "    print(\"\\nTraining Model 3: GRU Network (PyTorch)...\")\n",
    "    \n",
    "    print(\"Processing data for GRU...\")\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    n_features = X_train_processed.shape[1]\n",
    "    \n",
    "    X_train_seq, y_train_seq = create_sequences(X_train_processed, y_train.values, LOOKBACK_DAYS)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test_processed, y_test.values, LOOKBACK_DAYS)\n",
    "    X_val_seq, y_val_seq = create_sequences(X_val_processed, y_val.values, LOOKBACK_DAYS)\n",
    "    \n",
    "    print(f\"Created {X_train_seq.shape[0]} training sequences.\")\n",
    "    print(f\"Created {X_test_seq.shape[0]} test sequences.\")\n",
    "    \n",
    "\n",
    "    if X_train_seq.shape[0] == 0 or X_test_seq.shape[0] == 0:\n",
    "        print(\"WARNING: Not enough data to train or test GRU model. Skipping.\")\n",
    "    else:\n",
    "        train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        val_dataset = TensorDataset(X_test_seq, y_test_seq)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        test_pred_dataset = TensorDataset(X_test_seq)\n",
    "        test_pred_loader = DataLoader(test_pred_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        model_gru = GRUModel(input_features=n_features).to(device)\n",
    "        \n",
    "        pos_weight_tensor = torch.tensor(scale_pos_weight, dtype=torch.float32).to(device)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "        optimizer = optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        train_pytorch_model(\n",
    "            model_gru,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            EPOCHS,\n",
    "            PATIENCE,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        y_pred_gru, y_prob_gru = predict_pytorch_model(model_gru, test_pred_loader, device)\n",
    "        evaluate_model(\"GRU (Test Set)\", y_test_seq, y_pred_gru, y_prob_gru)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Final Validation on Holdout Set ---\")\n",
    "    print(\"This is the 'true' unseen performance of your best model.\")\n",
    "    \n",
    "    y_pred_val_xgb = xgb_pipeline.predict(X_val)\n",
    "    y_prob_val_xgb = xgb_pipeline.predict_proba(X_val)[:, 1]\n",
    "    evaluate_model(\"XGBoost (FINAL VALIDATION)\", y_val, y_pred_val_xgb, y_prob_val_xgb)\n",
    "    \n",
    "    if 'model_gru' in locals() and X_val_seq.shape[0] > 0:\n",
    "        val_pred_dataset = TensorDataset(X_val_seq)\n",
    "        val_pred_loader = DataLoader(val_pred_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        y_pred_val_gru, y_prob_val_gru = predict_pytorch_model(model_gru, val_pred_loader, device)\n",
    "        evaluate_model(\"GRU (FINAL VALIDATION)\", y_val_seq, y_pred_val_gru, y_prob_val_gru)\n",
    "    elif 'model_gru' in locals():\n",
    "         print(\"\\nWARNING: Not enough data in validation set for GRU sequence. Skipping.\")\n",
    "    elif 'model_gru' in locals():\n",
    "         print(\"\\nWARNING: Not enough data in validation set for GRU sequence. Skipping.\")\n",
    "\n",
    "    MODEL_DIR = 'final_model_checkpoints'\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        X_train_processed_temp = preprocessor.transform(X_train.head())\n",
    "        n_features = X_train_processed_temp.shape[1]\n",
    "        print(f\"\\nModel has {n_features} input features.\")\n",
    "        with open(os.path.join(MODEL_DIR, 'n_features.txt'), 'w') as f:\n",
    "             f.write(str(n_features))\n",
    "        print(f\"Saved feature count ({n_features}) to {MODEL_DIR}/n_features.txt\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save n_features: {e}\")\n",
    "\n",
    "    if 'model_gru' not in locals():\n",
    "        model_gru = None\n",
    "        \n",
    "    print(\"Returning trained models...\")\n",
    "    return logit_pipeline, xgb_pipeline, model_gru, preprocessor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccd943-76b7-423a-8d6a-58eef207d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting model training...\")\n",
    "\n",
    "MODEL_DIR = 'final_model_checkpoints' \n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "logit_pipeline, xgb_pipeline, model_gru, preprocessor = main()\n",
    "\n",
    "print(f\"\\nTraining complete. Saving models to {MODEL_DIR}...\")\n",
    "\n",
    "try:\n",
    "    joblib.dump(logit_pipeline, os.path.join(MODEL_DIR, 'logit_pipeline.joblib'))\n",
    "    joblib.dump(xgb_pipeline, os.path.join(MODEL_DIR, 'xgb_pipeline.joblib'))\n",
    "    joblib.dump(preprocessor, os.path.join(MODEL_DIR, 'preprocessor.joblib'))\n",
    "    print(\"Saved Logit, XGBoost, and Preprocessor pipelines.\")\n",
    "    \n",
    "    if model_gru:\n",
    "        torch.save(model_gru.state_dict(), os.path.join(MODEL_DIR, 'gru_model.pth'))\n",
    "        print(\"Saved GRU model state dictionary.\")\n",
    "    else:\n",
    "        print(\"GRU model was not trained, skipping save.\")\n",
    "        \n",
    "    print(\"\\nAll models and artifacts saved successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dcce6-7661-4e45-9d45-250b2de91a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "MODEL_DIR = 'final_model_checkpoints'\n",
    "LOOKBACK_DAYS = 5 \n",
    "BATCH_SIZE = 8\n",
    "TARGET_COL = 'target' \n",
    "GCP_PROJECT_ID = 'pivotal-glider-472219-r7' \n",
    "BIGQUERY_DATASET = 'market_data'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    bq_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
    "    print(f\"BigQuery client initialized for project {GCP_PROJECT_ID}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not initialize BigQuery client. {e}\")\n",
    "\n",
    "def create_table_if_not_exists(table_name, schema, partition_field):\n",
    "    table_id = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\"\n",
    "    try:\n",
    "        bq_client.get_table(table_id)\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_id} not found. Creating...\")\n",
    "        try:\n",
    "            table = bigquery.Table(table_id, schema=schema)\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field,\n",
    "            )\n",
    "            bq_client.create_table(table)\n",
    "            print(f\"  Success. Table {table_id} created.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  --- ERROR creating table {table_name} ---: {e}\")\n",
    "            raise\n",
    "\n",
    "def upload_to_bigquery(df, table_name, schema, partition_field='date'):\n",
    "    table_id = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\"\n",
    "    try:\n",
    "        create_table_if_not_exists(table_name, schema, partition_field)\n",
    "    except Exception as e:\n",
    "        print(f\"Aborting upload to {table_name} due to table creation error.\")\n",
    "        return\n",
    "    \n",
    "    df_upload = df.copy()\n",
    "    if df_upload.empty:\n",
    "        print(f\"No data provided for {table_name}, skipping upload.\")\n",
    "        return\n",
    "\n",
    "    df_upload[partition_field] = pd.to_datetime(df_upload[partition_field])\n",
    "\n",
    "    for col_schema in schema:\n",
    "        col_name = col_schema.name\n",
    "        if col_name not in df_upload.columns:\n",
    "            continue\n",
    "        if col_schema.field_type == 'NUMERIC':\n",
    "            df_upload[col_name] = pd.to_numeric(df_upload[col_name]).round(4)\n",
    "        elif col_schema.field_type == 'INTEGER':\n",
    "             df_upload[col_name] = df_upload[col_name].astype(float).astype('Int64')\n",
    "             \n",
    "    df_upload = df_upload.fillna(pd.NA).where(pd.notna(df_upload), None).replace({np.nan: None})\n",
    "    \n",
    "    print(f\"Uploading {len(df_upload)} rows to {table_name} (replacing all data)...\")\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        time_partitioning=bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=partition_field,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        job = bq_client.load_table_from_dataframe(df_upload, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "        print(f\"Success. Loaded {job.output_rows} rows into {table_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- FATAL ERROR uploading {table_name} ---\")\n",
    "        print(f\"  {e}\")\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        train_df = pd.read_csv('train_data.csv')\n",
    "        test_df = pd.read_csv('test_data.csv')\n",
    "        val_df = pd.read_csv('validation_data.csv')\n",
    "        \n",
    "        X_train = train_df.drop(columns=[TARGET_COL])\n",
    "        y_train = train_df[TARGET_COL]\n",
    "        X_test = test_df.drop(columns=[TARGET_COL])\n",
    "        y_test = test_df[TARGET_COL]\n",
    "        X_val = val_df.drop(columns=[TARGET_COL])\n",
    "        y_val = val_df[TARGET_COL]\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Cannot load data splits.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_features, gru_units=32, dropout_rate=0.3):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_features,\n",
    "            hidden_size=gru_units,\n",
    "            batch_first=True \n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.dense = nn.Linear(gru_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, hidden = self.gru(x)\n",
    "        last_hidden_state = hidden.squeeze(0)\n",
    "        x = self.relu(last_hidden_state)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "def create_sequences(X: np.ndarray, y: np.ndarray, lookback: int):\n",
    "    X_seq, y_seq = [], []\n",
    "    if len(X) <= lookback:\n",
    "        return torch.empty(0, lookback, X.shape[1]), torch.empty(0)\n",
    "    for i in range(lookback, len(X)):\n",
    "        X_seq.append(X[i-lookback:i, :])\n",
    "        y_seq.append(y[i])\n",
    "    X_seq_np = np.array(X_seq, dtype=np.float32)\n",
    "    y_seq_np = np.array(y_seq, dtype=np.float32)\n",
    "    return torch.from_numpy(X_seq_np), torch.from_numpy(y_seq_np)\n",
    "\n",
    "def predict_pytorch_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch in loader:\n",
    "            X_batch = X_batch[0].to(device)\n",
    "            y_logits = model(X_batch)\n",
    "            y_probs = torch.sigmoid(y_logits).squeeze()\n",
    "            if y_probs.dim() == 0:\n",
    "                all_probs.append(y_probs.cpu().numpy().reshape(1))\n",
    "            else:\n",
    "                all_probs.append(y_probs.cpu().numpy())\n",
    "    if not all_probs:\n",
    "        return np.array([]), np.array([])\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_preds = (all_probs > 0.5).astype(int)\n",
    "    return all_preds, all_probs\n",
    "\n",
    "print(\"\\nLoading all models and preprocessor...\")\n",
    "try:\n",
    "    logit_pipeline = joblib.load(os.path.join(MODEL_DIR, 'logit_pipeline.joblib'))\n",
    "    xgb_pipeline = joblib.load(os.path.join(MODEL_DIR, 'xgb_pipeline.joblib'))\n",
    "    preprocessor = joblib.load(os.path.join(MODEL_DIR, 'preprocessor.joblib'))\n",
    "    \n",
    "    with open(os.path.join(MODEL_DIR, 'n_features.txt'), 'r') as f:\n",
    "        n_features = int(f.read())\n",
    "        \n",
    "    print(f\"Loading GRU model with {n_features} input features.\")\n",
    "    model_gru = GRUModel(input_features=n_features).to(device)\n",
    "    model_gru.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'gru_model.pth'), map_location=device))\n",
    "    model_gru.eval()\n",
    "    \n",
    "    print(\"All models loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"FATAL: Could not load models. {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading all data splits...\")\n",
    "    data = load_data()\n",
    "    if data[0] is None:\n",
    "        print(\"Could not load data. Aborting.\")\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test, X_val, y_val = data\n",
    "        \n",
    "        train_df_full = X_train.assign(target=y_train)\n",
    "        test_df_full = X_test.assign(target=y_test)\n",
    "        val_df_full = X_val.assign(target=y_val)\n",
    "        \n",
    "        all_df = pd.concat([train_df_full, test_df_full, val_df_full], axis=0)\n",
    "        \n",
    "        all_df = all_df.sort_values(by='date').reset_index(drop=True)\n",
    "        \n",
    "        X_all = all_df.drop(columns=[TARGET_COL])\n",
    "        y_all = all_df[TARGET_COL]\n",
    "        \n",
    "        all_dates = X_all['date']\n",
    "        print(f\"Loaded {len(X_all)} total data points.\")\n",
    "\n",
    "        print(\"Running Logistic Regression predictions...\")\n",
    "        y_pred_logit = logit_pipeline.predict(X_all)\n",
    "        \n",
    "        print(\"Running XGBoost predictions...\")\n",
    "        y_pred_xgb = xgb_pipeline.predict(X_all)\n",
    "        \n",
    "        print(\"Running GRU predictions...\")\n",
    "        X_all_processed = preprocessor.transform(X_all)\n",
    "        X_all_seq, y_all_seq_truth = create_sequences(X_all_processed, y_all.values, LOOKBACK_DAYS)\n",
    "        \n",
    "        all_pred_dataset = TensorDataset(X_all_seq)\n",
    "        all_pred_loader = DataLoader(all_pred_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        y_pred_gru, _ = predict_pytorch_model(model_gru, all_pred_loader, device)\n",
    "\n",
    "        print(\"Assembling results dataframe...\")\n",
    "        df_results = pd.DataFrame({\n",
    "            'date': all_dates,\n",
    "            'ground_truth_value': y_all\n",
    "        })\n",
    "        \n",
    "        df_results['logit_pred'] = y_pred_logit\n",
    "        df_results['xgboost_pred'] = y_pred_xgb\n",
    "        \n",
    "        gru_preds_series = pd.Series(y_pred_gru, index=df_results.index[LOOKBACK_DAYS:])\n",
    "        df_results['gru_pred'] = gru_preds_series\n",
    "        \n",
    "        advice_map = {0: 'Sell', 1: 'Buy', np.nan: None}\n",
    "        \n",
    "        df_results['ground_truth_advice'] = df_results['ground_truth_value'].map(advice_map)\n",
    "        df_results['logit_advice'] = df_results['logit_pred'].map(advice_map)\n",
    "        df_results['xgboost_advice'] = df_results['xgboost_pred'].map(advice_map)\n",
    "        df_results['gru_advice'] = df_results['gru_pred'].map(advice_map)\n",
    "\n",
    "        df_results_final = df_results[[\n",
    "            'date',\n",
    "            'ground_truth_value',\n",
    "            'ground_truth_advice',\n",
    "            'logit_advice',\n",
    "            'xgboost_advice',\n",
    "            'gru_advice'\n",
    "        ]]\n",
    "        \n",
    "        print(\"\\nPrediction Results (head):\")\n",
    "        print(df_results_final.head(10))\n",
    "        print(\"\\nPrediction Results (tail):\")\n",
    "        print(df_results_final.tail())\n",
    "\n",
    "        print(\"\\nUploading results to BigQuery table 'model_results'...\")\n",
    "        \n",
    "        results_schema = [\n",
    "            bigquery.SchemaField('date', 'DATE'),\n",
    "            bigquery.SchemaField('ground_truth_value', 'INTEGER'),\n",
    "            bigquery.SchemaField('ground_truth_advice', 'STRING'),\n",
    "            bigquery.SchemaField('logit_advice', 'STRING'),\n",
    "            bigquery.SchemaField('xgboost_advice', 'STRING'),\n",
    "            bigquery.SchemaField('gru_advice', 'STRING'),\n",
    "        ]\n",
    "        \n",
    "        upload_to_bigquery(\n",
    "            df_results_final,\n",
    "            'model_results',\n",
    "            results_schema,\n",
    "            partition_field='date'\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Process Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU] Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
