{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999ea2c-8b1d-4e20-9228-8d7669193259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import numpy as np\n",
    "\n",
    "class CustomSentimentClassifierWithWeights(nn.Module):\n",
    "    def __init__(self, base_model, class_weights, num_labels=3, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "        dropped_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(dropped_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "def load_model_and_tokenizer(model_path=\"sentiment_model/article_sentiment_analysis.pth\", base_checkpoint=\"distilbert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Loads the trained model, tokenizer, and sets up the device.\n",
    "    \"\"\"\n",
    "    print(\"Loading sentiment model and tokenizer...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_checkpoint)\n",
    "\n",
    "    base_model = AutoModel.from_pretrained(base_checkpoint).to(device)\n",
    "\n",
    "    dummy_weights = torch.ones(3).to(device) \n",
    "    \n",
    "    model = CustomSentimentClassifierWithWeights(\n",
    "        base_model=base_model,\n",
    "        class_weights=dummy_weights,\n",
    "        num_labels=3\n",
    "    )\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Model file not found at {model_path}\")\n",
    "        print(\"Please make sure the file is in the same directory.\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state_dict: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"sentiment model loaded successfully.\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def get_sentiment(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Gets a single sentiment score [-1, 1] for a given text\n",
    "    using the loaded custom model.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    score = probs[2] - probs[0]\n",
    "    \n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee513a-bf5e-413b-9f11-ec128db0ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from dateutil.parser import parse as _parse \n",
    "\n",
    "\n",
    "GCP_PROJECT_ID = 'pivotal-glider-472219-r7'\n",
    "BIGQUERY_DATASET = 'market_data'\n",
    "\n",
    "print(\"Initializing BigQuery client...\")\n",
    "try:\n",
    "    bq_client = bigquery.Client(project=GCP_PROJECT_ID)\n",
    "    dataset_ref = bq_client.dataset(BIGQUERY_DATASET)\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"BigQuery dataset {BIGQUERY_DATASET} already exists.\")\n",
    "except NotFound:\n",
    "    print(f\"BigQuery dataset {BIGQUERY_DATASET} not found. Creating...\")\n",
    "    try:\n",
    "        bq_client.create_dataset(dataset_ref, timeout=30)\n",
    "        print(f\"Dataset {BIGQUERY_DATASET} created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not create dataset. {e}\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not initialize BigQuery client. {e}\")\n",
    "    print(\"Please ensure you have authenticated with 'gcloud auth application-default login'\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "def create_table_if_not_exists(table_name, schema, partition_field):\n",
    "    \"\"\"\n",
    "    Creates a BQ table with partitioning if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    table_id = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        bq_client.get_table(table_id)\n",
    "    except NotFound:\n",
    "        print(f\"Table {table_id} not found. Creating...\")\n",
    "        try:\n",
    "            table = bigquery.Table(table_id, schema=schema)\n",
    "            table.time_partitioning = bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=partition_field,\n",
    "            )\n",
    "            bq_client.create_table(table)\n",
    "            print(f\"  Success. Table {table_id} created with partitioning on '{partition_field}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  --- ERROR creating table {table_name} ---\")\n",
    "            print(f\"  {e}\")\n",
    "            raise\n",
    "\n",
    "def upload_to_bigquery(df, table_name, schema, partition_field='date'):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to BigQuery idempotently using a DELETE/INSERT pattern.\n",
    "    \"\"\"\n",
    "    table_id = f\"{GCP_PROJECT_ID}.{BIGQUERY_DATASET}.{table_name}\"\n",
    "    \n",
    "   \n",
    "    try:\n",
    "        create_table_if_not_exists(table_name, schema, partition_field)\n",
    "    except Exception as e:\n",
    "        print(f\"Aborting upload to {table_name} due to table creation error.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    df_upload = df.copy()\n",
    "\n",
    "    if df_upload.empty:\n",
    "        print(f\"No data provided for {table_name}, skipping upload.\")\n",
    "        return\n",
    "\n",
    "    if partition_field not in df_upload.columns:\n",
    "        print(f\"ERROR: Partition field '{partition_field}' not in DataFrame columns: {df_upload.columns}\")\n",
    "        return\n",
    "    df_upload[partition_field] = pd.to_datetime(df_upload[partition_field]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    for col_schema in schema:\n",
    "        col_name = col_schema.name\n",
    "        if col_name not in df_upload.columns:\n",
    "            continue\n",
    "        \n",
    "        if col_schema.field_type == 'NUMERIC':\n",
    "            df_upload[col_name] = pd.to_numeric(df_upload[col_name]).round(4)\n",
    "        elif col_schema.field_type == 'INTEGER':\n",
    "             df_upload[col_name] = df_upload[col_name].astype(float).astype('Int64')\n",
    "\n",
    "    df_upload = df_upload.fillna(pd.NA).where(pd.notna(df_upload), None)\n",
    "    df_upload = df_upload.replace({np.nan: None})\n",
    "\n",
    "    # --- 3. Delete Existing Data in Date Range ---\n",
    "    min_date = df_upload[partition_field].min()\n",
    "    max_date = df_upload[partition_field].max()\n",
    "\n",
    "    if pd.isna(min_date) or pd.isna(max_date):\n",
    "        print(f\"  No valid dates found in data for {table_name}, skipping delete and insert.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nMaking upload idempotent: Deleting existing data in {table_name} between {min_date} and {max_date}...\")\n",
    "    \n",
    "    try:\n",
    "        query_params = [\n",
    "            bigquery.ScalarQueryParameter(\"min_date\", \"DATE\", min_date),\n",
    "            bigquery.ScalarQueryParameter(\"max_date\", \"DATE\", max_date),\n",
    "        ]\n",
    "        job_config = bigquery.QueryJobConfig(query_parameters=query_params)\n",
    "        \n",
    "        delete_query = f\"\"\"\n",
    "            DELETE FROM `{table_id}`\n",
    "            WHERE {partition_field} BETWEEN @min_date AND @max_date\n",
    "        \"\"\"\n",
    "        \n",
    "        delete_job = bq_client.query(delete_query, job_config=job_config)\n",
    "        delete_job.result()\n",
    "        \n",
    "        print(f\"  Success. Deleted {delete_job.num_dml_affected_rows} existing rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  --- ERROR during DELETE operation for {table_name} ---\")\n",
    "        print(f\"  {e}\")\n",
    "        print(f\"  Aborting upload for this table.\")\n",
    "        return\n",
    "\n",
    "   \n",
    "    rows_to_insert = df_upload.to_dict('records')\n",
    "\n",
    "    if not rows_to_insert:\n",
    "        print(f\"  No data to upload for {table_name} after processing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Uploading {len(rows_to_insert)} new rows to {table_id} (using streaming insert)...\")\n",
    "    \n",
    "    try:\n",
    "        errors = bq_client.insert_rows_json(table_id, rows_to_insert)\n",
    "        \n",
    "        if not errors:\n",
    "            print(f\"  Success. Streamed data.\")\n",
    "        else:\n",
    "            print(f\"  --- ERROR uploading {table_name} ---\")\n",
    "            print(\"  Errors encountered during streaming insert:\")\n",
    "            for i, error in enumerate(errors):\n",
    "                if i < 20:\n",
    "                    print(f\"  - Row {error['index']}: {error['errors']}\")\n",
    "                elif i == 20:\n",
    "                    print(f\"  ... and {len(errors) - 20} more errors.\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"  --- FATAL ERROR uploading {table_name} ---\")\n",
    "        print(f\"  {e}\")\n",
    "        print(\"  DataFrame Info:\")\n",
    "        df_upload.info()\n",
    "        print(\"\\n  DataFrame Head:\")\n",
    "        print(df_upload.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd9b29-2f39-4baf-858b-6fbb3e6441eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "\n",
    "if model is None:\n",
    "    print(\"Failed to load model, exiting pipeline.\")\n",
    "    exit()\n",
    "\n",
    "ASPECT_MAP = {\n",
    "    \"interest_rates\": [\n",
    "        \"interest rate\", \"interest rates\", \"rate cuts\", \"rate cut\", \"fed rate cuts\",\n",
    "        \"federal reserve\", \"central bank\", \"chair jerome powell\", \"fed\", \"rate\",\n",
    "        \"federal open market\", \"open market committee\", \"lower interest rates\"\n",
    "    ],\n",
    "    \"inflation\": [\n",
    "        \"inflation\", \"consumer price index\", \"cpi\", \"producer price index\", \"ppi\",\n",
    "        \"personal consumption expenditures\", \"rising prices\", \"cost of living\",\n",
    "        \"price\", \"prices\", \"consumer\", \"spending\"\n",
    "    ],\n",
    "    \"unemployment\": [\n",
    "        \"unemployment\", \"labor market\", \"bureau labor statistics\", \"labor department report\",\n",
    "        \"nonfarm payrolls\", \"jobless claims\", \"job growth\", \"layoffs\", \"job\", \"jobs\", \"labor\"\n",
    "    ],\n",
    "    \"stock_market\": [\n",
    "        \"stock market\", \"wall street\", \"dow jones\", \"dow jones industrial\", \"jones industrial average\",\n",
    "        \"nasdaq composite\", \"stock exchange\", \"new york stock\", \"market cap\", \"per share\",\n",
    "        \"price target\", \"earnings per share\", \"stock\", \"stocks\", \"shares\", \"market\"\n",
    "    ],\n",
    "    \"recession\": [\n",
    "        \"recession\", \"economic downturn\", \"negative growth\", \"gdp\", \"economy\", \"growth\",\n",
    "        \"great financial crisis\"\n",
    "    ],\n",
    "    \"tech_ai\": [\n",
    "        \"artificial intelligence\", \"ai\", \"meta\", \"nvidia\", \"google\", \"apple\", \"microsoft\",\n",
    "        \"meta ray ban\", \"ray ban\", \"smart glasses\", \"meta smart glasses\", \"ceo mark zuckerberg\",\n",
    "        \"machine learning\", \"data center\", \"data centers\", \"ai models\", \"scale ai\", \"openai\",\n",
    "        \"meta superintelligence labs\", \"large language models\", \"nvidia ceo jensen\"\n",
    "    ],\n",
    "    \"politics_regulation\": [\n",
    "        \"donald trump\", \"president donald trump\", \"white house\", \"trump administration\",\n",
    "        \"federal trade commission\", \"securities exchange commission\", \"house oversight committee\",\n",
    "        \"government\", \"tariffs\", \"china\", \"chinese\", \"u k\", \"trade\"\n",
    "    ],\n",
    "    \"finance_banking\": [\n",
    "        \"goldman sachs\", \"morgan stanley\", \"bank america\", \"jpmorgan chase\", \"ceo jamie dimon\",\n",
    "        \"private equity\", \"financial\", \"investment\", \"investors\", \"fund\", \"funds\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "CATEGORIES = list(ASPECT_MAP.keys())\n",
    "\n",
    "print(\"\\nStarting Category-Based Sentiment Analysis with custom model...\")\n",
    "try:\n",
    "    df_articles = pd.read_json('articles_export.json')\n",
    "    print(f\"Loaded {len(df_articles)} articles.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'articles_export.json' not found.\")\n",
    "    exit()\n",
    "\n",
    "df_articles['date'] = pd.to_datetime(df_articles['published_at']).dt.date\n",
    "\n",
    "all_category_sentiments = []\n",
    "start_time = time.time()\n",
    "\n",
    "for index, article in df_articles.iterrows():\n",
    "    if not isinstance(article['article_content'], str):\n",
    "        continue\n",
    "\n",
    "    article_date = article['date']\n",
    "    article_id = article.get('id', index)\n",
    "    article_content = article['article_content']\n",
    "    article_content_lower = article_content.lower()\n",
    "\n",
    "    article_sentiment_score = get_sentiment(article_content, model, tokenizer, device)\n",
    "\n",
    "    for category, keywords in ASPECT_MAP.items():\n",
    "\n",
    "        if any(keyword in article_content_lower for keyword in keywords):\n",
    "\n",
    "            all_category_sentiments.append({\n",
    "                \"date\": article_date,\n",
    "                \"article_id\": article_id,\n",
    "                \"category\": category,\n",
    "                \"sentiment_score\": article_sentiment_score\n",
    "            })\n",
    "\n",
    "    if (index + 1) % 100 == 0:\n",
    "        processed_count = index + 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        articles_per_sec = processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        print(f\"  ...processed {processed_count} / {len(df_articles)} articles ({articles_per_sec:.2f} art/sec)\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Finished processing all articles in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "if not all_category_sentiments:\n",
    "    print(\"No categories were found in any articles. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nAggregating sentiment scores by day...\")\n",
    "df_category_scores = pd.DataFrame(all_category_sentiments)\n",
    "\n",
    "daily_category_sentiment = df_category_scores.groupby(['date', 'category'])['sentiment_score'].mean()\n",
    "\n",
    "df_final_sentiment = daily_category_sentiment.unstack(level='category')\n",
    "\n",
    "df_final_sentiment.columns = [f\"sentiment_{col}\" for col in df_final_sentiment.columns]\n",
    "\n",
    "news_volume = df_articles.groupby('date').size().to_frame('news_volume')\n",
    "df_final_sentiment = df_final_sentiment.join(news_volume)\n",
    "df_final_sentiment = df_final_sentiment.fillna(0)\n",
    "df_final_sentiment = df_final_sentiment.reset_index()\n",
    "\n",
    "print(\"Daily Category-Based Sentiment is complete:\")\n",
    "print(df_final_sentiment.head())\n",
    "\n",
    "print(\"\\nPreparing schema and data for BigQuery...\")\n",
    "absa_sentiment_schema = [\n",
    "    bigquery.SchemaField('date', 'DATE'),\n",
    "    bigquery.SchemaField('news_volume', 'INTEGER'),\n",
    "]\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    col_name = f\"sentiment_{category}\"\n",
    "\n",
    "    if col_name not in df_final_sentiment.columns:\n",
    "         print(f\"Warning: Category '{category}' not found in any articles. Adding empty column.\")\n",
    "         df_final_sentiment[col_name] = 0.0\n",
    "\n",
    "    absa_sentiment_schema.append(\n",
    "        bigquery.SchemaField(col_name, 'NUMERIC')\n",
    "    )\n",
    "\n",
    "schema_names = [field.name for field in absa_sentiment_schema]\n",
    "try:\n",
    "    df_final_sentiment = df_final_sentiment[schema_names]\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Mismatch between schema and DataFrame columns. Missing: {e}\")\n",
    "    print(f\"DataFrame columns: {df_final_sentiment.columns.tolist()}\")\n",
    "    print(f\"Expected schema columns: {schema_names}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Uploading to BigQuery...\")\n",
    "upload_to_bigquery(\n",
    "    df_final_sentiment,\n",
    "    'daily_sentiment_absa',\n",
    "    absa_sentiment_schema,\n",
    "    partition_field='date'\n",
    ")\n",
    "\n",
    "print(\"\\n\\nSuccessfully uploaded sentiment data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU] Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
