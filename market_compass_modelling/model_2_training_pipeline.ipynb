{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df4a8f-e086-4682-90dc-9441955783a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "PROJECT_ID = \"pivotal-glider-472219-r7\"\n",
    "TABLE_ID = \"pivotal-glider-472219-r7.market_data.master_data\"\n",
    "TRAIN_RATIO = 0.80\n",
    "TEST_RATIO = 0.15\n",
    "VALIDATION_RATIO = 1.0 - TRAIN_RATIO - TEST_RATIO\n",
    "\n",
    "def get_data_from_bigquery(project_id: str, table_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads the entire master_data table from BigQuery, ordered by date.\n",
    "    \n",
    "    NOTE: Make sure you are authenticated!\n",
    "    Run `gcloud auth application-default login` in your terminal.\n",
    "    \"\"\"\n",
    "    print(f\"Connecting to BigQuery project '{project_id}'...\")\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    \n",
    "    # Query to select all data ordered by date\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{table_id}`\n",
    "    ORDER BY date ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Downloading data... This may take a moment.\")\n",
    "    try:\n",
    "        query_job = client.query(query)\n",
    "        df = query_job.to_dataframe(create_bqstorage_client=True, dtypes={\"fed_Funds\": \"float64\", \"dgs_10\": \"float64\"})\n",
    "        print(f\"Successfully downloaded {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        print(\"Please ensure your project/table ID is correct and you have authenticated.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Prepares the raw DataFrame for ML:\n",
    "    1. Converts 'date' to datetime.\n",
    "    2. Creates 'reference_price' (last valid close, e.g., Friday's price for Fri, Sat, Sun).\n",
    "    3. Creates a 'target_price' column (next valid market close, e.g., Monday's price for Fri, Sat, Sun).\n",
    "    4. Creates the binary 'target' variable based on 'target_price' vs. 'reference_price'.\n",
    "    5. Drops rows that still have NaNs in target or reference (start/end of dataset).\n",
    "    6. Forward-fills/backward-fills missing *feature* data.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    print(\"Starting preprocessing...\")\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # 2. Create 'reference_price': The last known market close.\n",
    "    #    For Fri, Sat, Sun, this will be Friday's price.\n",
    "    df['reference_price'] = df['sp500'].ffill()\n",
    "    \n",
    "    # 3. Create 'target_price': The next available market close.\n",
    "    #    Shift by -1 (to get Sat's 'null' for Fri), then bfill()\n",
    "    #    to pull Monday's price back to Fri, Sat, and Sun.\n",
    "    df['target_price'] = df['sp500'].shift(-1).bfill()\n",
    "\n",
    "    # 4. Drop rows where we couldn't determine a reference or target.\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna(subset=['reference_price', 'target_price']).copy()\n",
    "    if initial_rows > len(df):\n",
    "        print(f\"Dropped {initial_rows - len(df)} rows from start/end of dataset (no reference or target).\")\n",
    "\n",
    "    # 5. Create the target variable\n",
    "    #    Target = 1 ('buy') if next market day's price >= last market day's price\n",
    "    #    Target = 0 ('sell') if next market day's price < last market day's price\n",
    "    df['target'] = (df['target_price'] >= df['reference_price']).astype(int)\n",
    "    \n",
    "    # 6. Handle missing feature data (e.g., 'fed_Funds', 'dgs_10' on holidays)\n",
    "    #    We forward-fill first, assuming values persist until a new one is reported.\n",
    "    #    Then, backward-fill to catch any NaNs at the very beginning of the dataset.\n",
    "    print(\"Filling missing feature data (ffill/bfill)...\")\n",
    "    \n",
    "    core_cols = ['date', 'target', 'sp500', 'reference_price', 'target_price']\n",
    "    feature_cols = [col for col in df.columns if col not in core_cols]\n",
    "    \n",
    "    df.loc[:, feature_cols] = df[feature_cols].ffill().bfill()\n",
    "    \n",
    "    # 7. Final cleanup\n",
    "    df = df.rename(columns={'reference_price': 'sp500_last_close'})\n",
    "    \n",
    "    helper_cols_to_drop = ['sp500', 'target_price']\n",
    "    \n",
    "    print(f\"Preprocessing complete. Final dataset shape: {df.shape}\")\n",
    "    return df, helper_cols_to_drop\n",
    "\n",
    "def split_data_chronologically_by_week(df: pd.DataFrame, train_ratio: float, test_ratio: float) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Splits the data into train/test/validation sets based on chronological weeks.\n",
    "    This prevents data leakage by ensuring the validation set is the most recent\n",
    "    and the train set is the oldest.\n",
    "    \"\"\"\n",
    "    print(\"Splitting data chronologically by week...\")\n",
    "    \n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['year_week'] = df['year'].astype(str) + '-' + df['fiscal_week'].astype(str)\n",
    "    \n",
    "    unique_weeks = df['year_week'].unique()\n",
    "    n_weeks = len(unique_weeks)\n",
    "    \n",
    "    if n_weeks < 3:\n",
    "        print(f\"Warning: Only {n_weeks} unique weeks. Not enough data to split into train/test/val.\")\n",
    "        print(\"Returning all data as training set.\")\n",
    "        helper_cols = ['year', 'year_week']\n",
    "        df = df.drop(columns=helper_cols)\n",
    "        return df, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    val_count = int(np.ceil(n_weeks * VALIDATION_RATIO))\n",
    "    test_count = int(np.ceil(n_weeks * test_ratio))\n",
    "    \n",
    "    if (val_count + test_count) >= n_weeks:\n",
    "        val_count = 1\n",
    "        test_count = 1\n",
    "        train_count = max(1, n_weeks - 2)\n",
    "    else:\n",
    "        train_count = n_weeks - val_count - test_count\n",
    "\n",
    "    train_split_idx = train_count\n",
    "    test_split_idx = train_count + test_count\n",
    "    \n",
    "    train_weeks = unique_weeks[:train_split_idx]\n",
    "    test_weeks = unique_weeks[train_split_idx:test_split_idx]\n",
    "    val_weeks = unique_weeks[test_split_idx:]\n",
    "    \n",
    "    print(f\"Total unique weeks: {n_weeks}\")\n",
    "    \n",
    "    if len(train_weeks) > 0:\n",
    "        print(f\"  - Train weeks: {len(train_weeks)} (Ending: {train_weeks[-1]})\")\n",
    "    else:\n",
    "        print(\"  - Train weeks: 0\")\n",
    "\n",
    "    if len(test_weeks) > 0:\n",
    "        print(f\"  - Test weeks: {len(test_weeks)} (Starting: {test_weeks[0]}, Ending: {test_weeks[-1]})\")\n",
    "    else:\n",
    "        print(\"  - Test weeks: 0\")\n",
    "\n",
    "    if len(val_weeks) > 0:\n",
    "        print(f\"  - Validation weeks: {len(val_weeks)} (Starting: {val_weeks[0]})\")\n",
    "    else:\n",
    "        print(\"  - Validation weeks: 0\")\n",
    "    \n",
    "    train_df = df[df['year_week'].isin(train_weeks)].copy()\n",
    "    test_df = df[df['year_week'].isin(test_weeks)].copy()\n",
    "    val_df = df[df['year_week'].isin(val_weeks)].copy()\n",
    "    \n",
    "    helper_cols = ['year', 'year_week']\n",
    "    train_df = train_df.drop(columns=helper_cols)\n",
    "    test_df = test_df.drop(columns=helper_cols)\n",
    "    val_df = val_df.drop(columns=helper_cols)\n",
    "    \n",
    "    print(\"\\nData split complete:\")\n",
    "    print(f\"  - Train set shape: {train_df.shape}\")\n",
    "    print(f\"  - Test set shape:  {test_df.shape}\")\n",
    "    print(f\"  - Val set shape:   {val_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the data processing pipeline.\n",
    "    \"\"\"\n",
    "    raw_df = get_data_from_bigquery(PROJECT_ID, TABLE_ID)\n",
    "    \n",
    "    if raw_df.empty:\n",
    "        print(\"Exiting due to data download failure.\")\n",
    "        return\n",
    "\n",
    "    processed_df, helper_cols = preprocess_data(raw_df)\n",
    "    \n",
    "    if processed_df.empty:\n",
    "        print(\"Exiting due to preprocessing failure.\")\n",
    "        return\n",
    "\n",
    "    train_df, test_df, val_df = split_data_chronologically_by_week(\n",
    "        processed_df, TRAIN_RATIO, TEST_RATIO\n",
    "    )\n",
    "    \n",
    "    train_df = train_df.drop(columns=helper_cols, errors='ignore')\n",
    "    test_df = test_df.drop(columns=helper_cols, errors='ignore')\n",
    "    val_df = val_df.drop(columns=helper_cols, errors='ignore')\n",
    "\n",
    "    try:\n",
    "        print(\"\\nSaving data to CSV files...\")\n",
    "        train_df.to_csv(\"train_data.csv\", index=False)\n",
    "        test_df.to_csv(\"test_data.csv\", index=False)\n",
    "        val_df.to_csv(\"validation_data.csv\", index=False)\n",
    "        print(\"Successfully saved train_data.csv, test_data.csv, and validation_data.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving files: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU] Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
